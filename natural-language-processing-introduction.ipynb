{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8558bb5",
   "metadata": {},
   "source": [
    "## Natural Language Processing with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "adfaa959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from helper_function import performance_metrics, compare_baseline_with_new_result, create_tensorboard_callback\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b8515",
   "metadata": {},
   "source": [
    "### 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f6b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Link folders\n",
    "\n",
    "train_df = pd.read_csv(\"data/nlp/train.csv\")\n",
    "test_df = pd.read_csv(\"data/nlp/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e00eaef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Shuffle training dataframe\n",
    "\n",
    "train_df_shuffled = train_df.sample(frac = 1, random_state = 42)\n",
    "\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe34b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 7613\n",
      "Total test samples: 3263\n",
      "Total samples: 10876\n"
     ]
    }
   ],
   "source": [
    "### How many total sample?\n",
    "\n",
    "print(f\"Total training samples: {len(train_df)}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")\n",
    "print(f\"Total samples: {len(train_df) + len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83f7614c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Plane Panic What kind of douchebag. Bubble Gum\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "@kunalkapoor Photo of the Day: Storm\n",
      "Chaser\n",
      "http://t.co/4WJy7seHmw\n",
      "#photography #pod\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "@KapoKekito on northgate by the taco truck that's fire.\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "DLH issues Hazardous Weather Outlook (HWO) http://t.co/WOzuBXRi2p\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "#reuters Twelve feared killed in Pakistani air ambulance helicopter crash http://t.co/ShzPyIQok5\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Visualize random training samples\n",
    "\n",
    "random_index = random.randint(0, len(train_df) - 5)\n",
    "\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index + 5].itertuples():\n",
    "    _, text, target = row\n",
    "    \n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fef15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split training data into training and validation sets\n",
    "\n",
    "train_sentence, val_sentence, train_label, val_label = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                            test_size = 0.1,\n",
    "                                                                            random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "488c1e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train sentence: 6851, length of train label: 6851\n",
      "Length of val sentence: 762, length of val label: 762\n"
     ]
    }
   ],
   "source": [
    "### Check the length of validation & training\n",
    "\n",
    "print(f\"Length of train sentence: {len(train_sentence)}, length of train label: {len(train_label)}\")\n",
    "print(f\"Length of val sentence: {len(val_sentence)}, length of val label: {len(val_label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d0c0280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first 10 training sentences and its label\n",
    "\n",
    "train_sentence[:10], train_label[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a153924",
   "metadata": {},
   "source": [
    "### 2. Converting text into numbers\n",
    "Tokenization - word-level tokenization, character-level tokenization, sub-word tokenization <br>\n",
    "Embeddings - own/ custom embedding, pre-learned embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7840d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example of text vectorization\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens = None,\n",
    "                                    standardize = \"lower_and_strip_punctuation\",\n",
    "                                    split = \"whitespace\",\n",
    "                                    ngrams = None,\n",
    "                                    output_mode = \"int\",\n",
    "                                    output_sequence_length = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "316347b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of tokens is 15\n"
     ]
    }
   ],
   "source": [
    "### What is average number of tokens (words)?\n",
    "\n",
    "print(f\"The average of tokens is {round(sum(len(i.split()) for i in train_sentence) / len(train_sentence))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b9aa4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set text vectorization with custom variables\n",
    "### Set max number of words to have in our vocabulary\n",
    "### Max length for the sequences\n",
    "\n",
    "max_vocab_length = 10000\n",
    "max_length = 15\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens = max_vocab_length,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2435e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit the text vectorizer to the training text\n",
    "\n",
    "text_vectorizer.adapt(train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93a0d995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create sample sentence and tokenize it\n",
    "### Check the output\n",
    "\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73fda714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Drones Under Fire: Officials Offer $75000 Reward Leading To Pilots Who Flew Over Wildfire http://t.co/d2vEppeh8S #photography #arts      \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[2487,  204,   42,  503, 2367,    1, 2743, 1508,    5, 2021,   65,\n",
       "        2996,   60,  146,    1]], dtype=int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Choose random sentence from the training dataset then tokenize it\n",
    "\n",
    "random_sentence = random.choice(train_sentence)\n",
    "\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nVectorized version:\")\n",
    "\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8687918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "### Get the unique words in the vocabulary\n",
    "\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5]\n",
    "bottom_5_words = words_in_vocab[-5:]\n",
    "\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\") \n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06403205",
   "metadata": {},
   "source": [
    "### 3. Creating embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe200d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating embedding layer\n",
    "\n",
    "embedding = layers.Embedding(input_dim = max_vocab_length,\n",
    "     output_dim = 128, embeddings_initializer = \"uniform\",\n",
    "     input_length = max_length, name = \"embedding_layer\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad98e3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Kai Forbath just demolished a weather station set up on a drill field with a missed field goal. Thing just exploded into metal bits.      \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.00811068,  0.03491307, -0.02548101, ..., -0.03320241,\n",
       "          0.02853728,  0.02550546],\n",
       "        [-0.00811068,  0.03491307, -0.02548101, ..., -0.03320241,\n",
       "          0.02853728,  0.02550546],\n",
       "        [-0.00519258,  0.03926153,  0.0130669 , ..., -0.01611619,\n",
       "         -0.00551615, -0.01114497],\n",
       "        ...,\n",
       "        [-0.01655617,  0.01649188,  0.04291462, ..., -0.01350659,\n",
       "          0.03089793,  0.04934624],\n",
       "        [ 0.03377423, -0.01212207, -0.03183881, ...,  0.01489768,\n",
       "         -0.04352232, -0.02405475],\n",
       "        [-0.03596265, -0.00616892,  0.02295513, ...,  0.01972618,\n",
       "         -0.04185466, -0.00833704]]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get a random sentence from training set\n",
    "### Embed the random sentence\n",
    "\n",
    "random_sentence = random.choice(train_sentence)\n",
    "\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nEmbedded version:\")\n",
    "\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cf0cd265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([-0.00811068,  0.03491307, -0.02548101,  0.04781481,  0.04361737,\n",
       "        0.03199318,  0.01940601, -0.0483266 , -0.005664  , -0.03757665,\n",
       "       -0.03235463,  0.04903879, -0.02028735,  0.02265917, -0.04359738,\n",
       "       -0.02064624, -0.00183594,  0.03911011, -0.00225415, -0.03699765,\n",
       "        0.02796933,  0.00077952,  0.03411959,  0.04131328, -0.01507081,\n",
       "       -0.02272201,  0.03807184,  0.01373016, -0.03292575,  0.00670286,\n",
       "        0.02325675, -0.03902855,  0.02357253, -0.03854655, -0.01321665,\n",
       "        0.03578268, -0.04914347,  0.0247624 , -0.02353262, -0.02438924,\n",
       "        0.0293134 , -0.04433916,  0.0002748 , -0.00192436, -0.03667526,\n",
       "       -0.02790846, -0.01435464, -0.01166171, -0.00105019,  0.00022941,\n",
       "        0.01524897, -0.00659596,  0.00104121, -0.03893106,  0.01025798,\n",
       "       -0.04031094,  0.03773138, -0.01118631,  0.03802359,  0.01509837,\n",
       "       -0.0435953 ,  0.0068249 ,  0.00203154, -0.01919363,  0.02477253,\n",
       "        0.01271181,  0.03136289,  0.01368251,  0.02027344,  0.04019214,\n",
       "       -0.04124909,  0.02635797,  0.03794011,  0.03979656,  0.000526  ,\n",
       "        0.03395757, -0.04457115,  0.01033998, -0.04275378, -0.00216036,\n",
       "        0.03651043,  0.0456156 , -0.00709723, -0.00631695, -0.04882243,\n",
       "        0.03254885, -0.02885954, -0.03828561, -0.04673164,  0.03612042,\n",
       "        0.03721705,  0.00057169,  0.04617593,  0.04546544,  0.0459561 ,\n",
       "       -0.00455385, -0.04663719,  0.00316979, -0.021453  ,  0.01093661,\n",
       "       -0.04076361,  0.00572257,  0.03433961, -0.04643206,  0.00733286,\n",
       "       -0.01753007, -0.02619151, -0.01634421, -0.03079932, -0.01356941,\n",
       "       -0.04474789,  0.04935134,  0.0383368 , -0.03190057,  0.01892928,\n",
       "       -0.02370831, -0.03454485, -0.03937995, -0.04544994,  0.03283961,\n",
       "        0.03098248, -0.04291561, -0.01303262, -0.00788747,  0.00839467,\n",
       "       -0.03320241,  0.02853728,  0.02550546], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Single token's embedding\n",
    "\n",
    "sample_embed[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8182de5b",
   "metadata": {},
   "source": [
    "### 4. Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0954f4",
   "metadata": {},
   "source": [
    "Model 1 - Naive Bayes (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e060ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize save directory location\n",
    "\n",
    "saved_dir_loc = \"model_log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3cd6afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Convert words to numbers using tfidf then model the text\n",
    "\n",
    "first_model = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "first_model.fit(train_sentence, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3f4b881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check performance metrics\n",
    "\n",
    "baseline_pred = first_model.predict(val_sentence)\n",
    "\n",
    "baseline_result = performance_metrics(y_true = val_label, y_pred = baseline_pred)\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522fffb0",
   "metadata": {},
   "source": [
    "Model 2 - Simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce5baa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_layer (Embedding  (None, 15, 128)           1280000   \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1280129 (4.88 MB)\n",
      "Trainable params: 1280129 (4.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Create one dimensional strings inputs \n",
    "input_layer = layers.Input(shape = (1,), dtype = \"string\")\n",
    "\n",
    "### Turn the input text into numbers\n",
    "x = text_vectorizer(input_layer)\n",
    "\n",
    "### Embedding the numerized numbers\n",
    "x = embedding(x)\n",
    "\n",
    "### Lower the dimensionality of the embedding\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "### Create the output layer for binary outputs \n",
    "output_layer = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "### Construct the model\n",
    "second_model = tf.keras.Model(input_layer, output_layer)\n",
    "\n",
    "second_model.compile(loss = \"binary_crossentropy\", optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = [\"accuracy\"])\n",
    "\n",
    "second_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53d56536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/simple_dense_model/20230924-025729\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 19ms/step - loss: 0.6094 - accuracy: 0.6996 - val_loss: 0.5346 - val_accuracy: 0.7664\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.4402 - accuracy: 0.8205 - val_loss: 0.4735 - val_accuracy: 0.7795\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 21ms/step - loss: 0.3458 - accuracy: 0.8609 - val_loss: 0.4572 - val_accuracy: 0.7927\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.2834 - accuracy: 0.8918 - val_loss: 0.4687 - val_accuracy: 0.7900\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 4s 20ms/step - loss: 0.2375 - accuracy: 0.9114 - val_loss: 0.4781 - val_accuracy: 0.7782\n"
     ]
    }
   ],
   "source": [
    "second_model_history = second_model.fit(train_sentence, train_label, epochs = 5,\n",
    "    validation_data = (val_sentence, val_label), \n",
    "    callbacks = [create_tensorboard_callback(dir_name = saved_dir_loc , experiment_name = \"simple_dense_model\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0143bb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step - loss: 0.4781 - accuracy: 0.7782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47811052203178406, 0.778215229511261]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check validation results\n",
    "\n",
    "second_model.evaluate(val_sentence, val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0f39266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_layer/embeddings:0' shape=(10000, 128) dtype=float32, numpy=\n",
       " array([[-3.7057016e-02,  4.8624083e-02,  1.4477805e-02, ...,\n",
       "          3.3055462e-02, -2.1986498e-02, -4.1537173e-02],\n",
       "        [-6.5777977e-03,  3.9635230e-02, -2.7613837e-02, ...,\n",
       "         -3.4749243e-02,  3.0362485e-02,  2.2442650e-02],\n",
       "        [ 3.9865156e-03,  6.0793847e-02,  3.5185479e-02, ...,\n",
       "         -6.2198386e-02,  4.2470168e-02, -2.3823939e-02],\n",
       "        ...,\n",
       "        [ 4.7637112e-03,  3.2985210e-04, -1.1808313e-02, ...,\n",
       "         -4.2228986e-02,  3.0479696e-02,  3.5024334e-02],\n",
       "        [ 8.9700632e-03,  5.1463611e-02,  6.8334718e-03, ...,\n",
       "         -4.5752746e-05,  4.5254495e-02, -1.4551225e-03],\n",
       "        [ 3.7684176e-02,  8.7184988e-02, -6.3494042e-02, ...,\n",
       "         -1.6940210e-02,  7.0671938e-02, -3.0954029e-02]], dtype=float32)>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check embedding weights\n",
    "\n",
    "embedding.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e70b260d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 128)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Other way to check embedding weights\n",
    "\n",
    "embed_weights = second_model.get_layer(\"embedding_layer\").get_weights()[0]\n",
    "\n",
    "embed_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b91198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard dev upload --logdir ./model_log \\\n",
    "#   --name \"First deep model on text data\" \\\n",
    "#   --description \"Trying a dense model with an embedding layer\" \\\n",
    "#   --one_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard dev delete --experiment_id EXPERIMENT_ID_TO_DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "669e98a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.82152230971128,\n",
       " 'precision': 0.7814103276314137,\n",
       " 'recall': 0.7782152230971129,\n",
       " 'f1': 0.7756075024838144}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_model_pred_prob = second_model.predict(val_sentence)\n",
    "\n",
    "### Turn into single-dimension tensor of float\n",
    "second_model_pred = tf.squeeze(tf.round(second_model_pred_prob))\n",
    "\n",
    "second_model_result = performance_metrics(y_true = val_label, y_pred = second_model_pred)\n",
    "\n",
    "second_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68ef5615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 77.82, Difference: -1.443569553805773\n",
      "Baseline precision: 0.81, New precision: 0.78, Difference: -0.029728672789903543\n",
      "Baseline recall: 0.79, New recall: 0.78, Difference: -0.01443569553805768\n",
      "Baseline f1: 0.79, New f1: 0.78, Difference: -0.010611473321140541\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_with_new_result(baseline_result = baseline_result, new_result = second_model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a64bcc",
   "metadata": {},
   "source": [
    "Model 3 - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cabe5aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"third_model_lstm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " third_embedding_layer (Emb  (None, 15, 128)           1280000   \n",
      " edding)                                                         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1329473 (5.07 MB)\n",
      "Trainable params: 1329473 (5.07 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "third_model_embedding = layers.Embedding(input_dim = max_vocab_length, output_dim = 128,\n",
    "    embeddings_initializer = \"uniform\", input_length = max_length, name = \"third_embedding_layer\")\n",
    "\n",
    "input_layer = layers.Input(shape = (1,), dtype = \"string\")\n",
    "\n",
    "x = text_vectorizer(input_layer)\n",
    "x = third_model_embedding(x)\n",
    "# print(x.shape)\n",
    "x = layers.LSTM(64)(x)\n",
    "# print(x.shape)\n",
    "\n",
    "output_layer = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "third_model = tf.keras.Model(input_layer, output_layer, name = \"third_model_lstm\")\n",
    "\n",
    "third_model.compile(loss = \"binary_crossentropy\", optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = [\"accuracy\"])\n",
    "\n",
    "third_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a81fdc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/lstm/20230924-030216\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 7s 26ms/step - loss: 0.5095 - accuracy: 0.7523 - val_loss: 0.4601 - val_accuracy: 0.7874\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.3136 - accuracy: 0.8739 - val_loss: 0.5171 - val_accuracy: 0.7808\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.2157 - accuracy: 0.9204 - val_loss: 0.5382 - val_accuracy: 0.7717\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1512 - accuracy: 0.9473 - val_loss: 0.6280 - val_accuracy: 0.7625\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1074 - accuracy: 0.9610 - val_loss: 0.9748 - val_accuracy: 0.7703\n"
     ]
    }
   ],
   "source": [
    "third_model_history = third_model.fit(train_sentence, train_label, epochs = 5, \n",
    "    validation_data = (val_sentence, val_label), callbacks = [create_tensorboard_callback(saved_dir_loc, \"lstm\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard dev upload --logdir ./model_logs \\\n",
    "#   --name \"First deep model on text data\" \\\n",
    "#   --description \"Trying a dense model with an embedding layer\" \\\n",
    "#   --one_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fce5ac14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.03412073490814,\n",
       " 'precision': 0.7729377683268125,\n",
       " 'recall': 0.7703412073490814,\n",
       " 'f1': 0.7677842762403819}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_model_pred_prob = third_model.predict(val_sentence)\n",
    "third_model_pred = tf.squeeze(tf.round(third_model_pred_prob))\n",
    "\n",
    "third_model_result = performance_metrics(y_true = val_label, y_pred = third_model_pred)\n",
    "third_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c13d57f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 77.03, Difference: -2.230971128608914\n",
      "Baseline precision: 0.81, New precision: 0.77, Difference: -0.03820123209450477\n",
      "Baseline recall: 0.79, New recall: 0.77, Difference: -0.022309711286089162\n",
      "Baseline f1: 0.79, New f1: 0.77, Difference: -0.018434699564573\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_with_new_result(baseline_result, third_model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a0d4a2",
   "metadata": {},
   "source": [
    "Model 4 - Bidirectonal RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1eeb2b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"forth_model_bidirectional_rnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " forth_embedding (Embedding  (None, 15, 128)           1280000   \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 128)               98816     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1378945 (5.26 MB)\n",
      "Trainable params: 1378945 (5.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "forth_model_embedding = layers.Embedding(input_dim = max_vocab_length, output_dim = 128,\n",
    "    embeddings_initializer = \"uniform\", input_length = max_length, name = \"forth_embedding\")\n",
    "\n",
    "input_layer = layers.Input(shape = (1,), dtype = \"string\")\n",
    "\n",
    "x = text_vectorizer(input_layer)\n",
    "x = forth_model_embedding(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "\n",
    "output_layer = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "forth_model = tf.keras.Model(input_layer, output_layer, name = \"forth_model_bidirectional_rnn\")\n",
    "\n",
    "forth_model.compile(loss = \"binary_crossentropy\", optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = [\"accuracy\"])\n",
    "\n",
    "forth_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "374f8f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/bidirectional_rnn/20230924-030319\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 8s 25ms/step - loss: 0.5101 - accuracy: 0.7440 - val_loss: 0.4599 - val_accuracy: 0.7861\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.3129 - accuracy: 0.8704 - val_loss: 0.5103 - val_accuracy: 0.7664\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.2106 - accuracy: 0.9223 - val_loss: 0.5354 - val_accuracy: 0.7664\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1426 - accuracy: 0.9526 - val_loss: 0.6924 - val_accuracy: 0.7756\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1038 - accuracy: 0.9638 - val_loss: 0.8695 - val_accuracy: 0.7677\n"
     ]
    }
   ],
   "source": [
    "forth_model_history = forth_model.fit(train_sentence, train_label,\n",
    "    epochs = 5, validation_data = (val_sentence, val_label), \n",
    "    callbacks = [create_tensorboard_callback(saved_dir_loc, \"bidirectional_rnn\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91198b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.77165354330708,\n",
       " 'precision': 0.7691343474319641,\n",
       " 'recall': 0.7677165354330708,\n",
       " 'f1': 0.7656749923220023}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forth_model_pred_prob = forth_model.predict(val_sentence)\n",
    "forth_model_pred = tf.squeeze(tf.round(forth_model_pred_prob))\n",
    "\n",
    "forth_model_result = performance_metrics(val_label, forth_model_pred)\n",
    "forth_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ee0fd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 76.77, Difference: -2.4934383202099752\n",
      "Baseline precision: 0.81, New precision: 0.77, Difference: -0.042004652989353186\n",
      "Baseline recall: 0.79, New recall: 0.77, Difference: -0.02493438320209973\n",
      "Baseline f1: 0.79, New f1: 0.77, Difference: -0.02054398348295261\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_with_new_result(baseline_result, forth_model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef67620",
   "metadata": {},
   "source": [
    "Model 5 - One dimensional cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2d69a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fifth_model_cnn_1d\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " fifth_embedding (Embedding  (None, 15, 128)           1280000   \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 11, 32)            20512     \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 32)                0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1300545 (4.96 MB)\n",
      "Trainable params: 1300545 (4.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fifth_model_embedding = layers.Embedding(input_dim = max_vocab_length, output_dim = 128,\n",
    "    embeddings_initializer = \"uniform\", input_length = max_length, name = \"fifth_embedding\")\n",
    "\n",
    "input_layer = layers.Input(shape = (1,), dtype = \"string\")\n",
    "\n",
    "x = text_vectorizer(input_layer)\n",
    "x = fifth_model_embedding(x)\n",
    "x = layers.Conv1D(filters = 32, kernel_size = 5, activation = \"relu\")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "\n",
    "output_layer = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "fifth_model = tf.keras.Model(input_layer, output_layer, name = \"fifth_model_cnn_1d\")\n",
    "\n",
    "fifth_model.compile(loss = \"binary_crossentropy\", \n",
    "    optimizer = tf.keras.optimizers.Adam(), metrics = [\"accuracy\"])\n",
    "\n",
    "fifth_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c23ee67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/cnn_1d/20230924-030406\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 19ms/step - loss: 0.5661 - accuracy: 0.7162 - val_loss: 0.4701 - val_accuracy: 0.7874\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 20ms/step - loss: 0.3432 - accuracy: 0.8575 - val_loss: 0.4649 - val_accuracy: 0.7992\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.2111 - accuracy: 0.9242 - val_loss: 0.5271 - val_accuracy: 0.7822\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.1360 - accuracy: 0.9545 - val_loss: 0.6067 - val_accuracy: 0.7822\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.0931 - accuracy: 0.9715 - val_loss: 0.6670 - val_accuracy: 0.7638\n"
     ]
    }
   ],
   "source": [
    "fifth_model_history = fifth_model.fit(train_sentence, train_label,\n",
    "    epochs = 5, validation_data = (val_sentence, val_label),\n",
    "    callbacks = [create_tensorboard_callback(saved_dir_loc, \"cnn_1d\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13be68ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.37795275590551,\n",
       " 'precision': 0.7638046232237288,\n",
       " 'recall': 0.7637795275590551,\n",
       " 'f1': 0.7625889751874003}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fifth_model_pred_prob = fifth_model.predict(val_sentence)\n",
    "fifth_model_pred = tf.squeeze(tf.round(fifth_model_pred_prob))\n",
    "\n",
    "fifth_model_result = performance_metrics(val_label, fifth_model_pred)\n",
    "fifth_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22dfc792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 76.38, Difference: -2.887139107611546\n",
      "Baseline precision: 0.81, New precision: 0.76, Difference: -0.047334377197588484\n",
      "Baseline recall: 0.79, New recall: 0.76, Difference: -0.02887139107611547\n",
      "Baseline f1: 0.79, New f1: 0.76, Difference: -0.023630000617554603\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_with_new_result(baseline_result, fifth_model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9687a4e",
   "metadata": {},
   "source": [
    "Model 6 - Pretrained sentence encoder <br><br>\n",
    "Difference between the embedding layer vs universal sentence encoder is rather than create a word-level embedding, the universal sentence encoder creates <b>whole sentence-level embedding</b>. Custom embedding layer outputs 128 dimensional vector for each word while universal sentence encoder outputs 512 dimensional vector for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af8968ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.01157023  0.02485909  0.02878049 -0.012715    0.0397154   0.08827759\n",
      "  0.02680983  0.05589837 -0.01068731 -0.00597293  0.00639322 -0.01819516\n",
      "  0.00030816  0.09105889  0.05874643 -0.03180629  0.01512472 -0.05162928\n",
      "  0.00991366 -0.06865345 -0.04209306  0.02678979  0.03011009  0.00321065\n",
      " -0.00337969 -0.04787359  0.0226672  -0.00985925 -0.04063614 -0.01292091\n",
      " -0.04666385  0.05630299 -0.03949255  0.00517684  0.02495827 -0.0701444\n",
      "  0.0287151   0.04947682 -0.00633976 -0.08960192  0.02807119 -0.00808363\n",
      " -0.01360601  0.05998649 -0.10361788 -0.05195374  0.00232955 -0.0233253\n",
      " -0.03758107  0.0332773 ], shape=(50,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "### Example of pretrained embedding with universal sentence encoder\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "embed_sample = embed([sample_sentence])\n",
    "\n",
    "### View first 50 \n",
    "print(embed_sample[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e2eb69a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### What is its shape?\n",
    "\n",
    "embed_sample[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8c56eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sixth_model_use\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256830721 (979.73 MB)\n",
      "Trainable params: 32897 (128.50 KB)\n",
      "Non-trainable params: 256797824 (979.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "    input_shape = [], dtype = tf.string, trainable = False, name = \"USE\")\n",
    "\n",
    "sixth_model = tf.keras.Sequential([\n",
    "  sentence_encoder_layer,\n",
    "  layers.Dense(64, activation = \"relu\"),\n",
    "  layers.Dense(1, activation = \"sigmoid\")\n",
    "], name = \"sixth_model_use\")\n",
    "\n",
    "sixth_model.compile(loss = \"binary_crossentropy\",\n",
    "    optimizer = tf.keras.optimizers.Adam(), metrics = [\"accuracy\"])\n",
    "\n",
    "sixth_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b7d12cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/tf_hub_sentence_encoder/20230924-032159\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 8s 23ms/step - loss: 0.5034 - accuracy: 0.7866 - val_loss: 0.4504 - val_accuracy: 0.7966\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 6s 30ms/step - loss: 0.4154 - accuracy: 0.8151 - val_loss: 0.4411 - val_accuracy: 0.8058\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 7s 31ms/step - loss: 0.4020 - accuracy: 0.8225 - val_loss: 0.4349 - val_accuracy: 0.8136\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 7s 31ms/step - loss: 0.3942 - accuracy: 0.8246 - val_loss: 0.4382 - val_accuracy: 0.8058\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 7s 31ms/step - loss: 0.3893 - accuracy: 0.8257 - val_loss: 0.4371 - val_accuracy: 0.8084\n"
     ]
    }
   ],
   "source": [
    "sixth_model_history = sixth_model.fit(train_sentence, train_label,\n",
    "    epochs = 5, validation_data = (val_sentence, val_label), \n",
    "    callbacks = [create_tensorboard_callback(saved_dir_loc, \"tf_hub_sentence_encoder\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9fb07507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 80.83989501312337,\n",
       " 'precision': 0.8124533883813974,\n",
       " 'recall': 0.8083989501312336,\n",
       " 'f1': 0.8062063003139296}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sixth_model_pred_prob = sixth_model.predict(val_sentence)\n",
    "sixth_model_pred = tf.squeeze(tf.round(sixth_model_pred_prob))\n",
    "\n",
    "sixth_model_result = performance_metrics(val_label, sixth_model_pred)\n",
    "sixth_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fc00c5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 80.84, Difference: 1.5748031496063106\n",
      "Baseline precision: 0.81, New precision: 0.81, Difference: 0.001314387960080099\n",
      "Baseline recall: 0.79, New recall: 0.81, Difference: 0.015748031496063075\n",
      "Baseline f1: 0.79, New f1: 0.81, Difference: 0.01998732450897467\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_with_new_result(baseline_result, sixth_model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d991f1",
   "metadata": {},
   "source": [
    "Model 7 - Pretrained sentence encoder with 10% data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4032f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence_90_percent, train_sentence_10_percent, train_label_90_percent, train_label_10_percent \\\n",
    "    = train_test_split(np.array(train_sentence), train_label, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "97e8c6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training: 6851\n",
      "Length of 10% training: 686\n",
      "Total label '0': 415\n",
      "Total label '1': 271\n"
     ]
    }
   ],
   "source": [
    "# Check length of 10 percent from training\n",
    "\n",
    "print(f\"Total training: {len(train_sentence)}\")\n",
    "print(f\"Length of 10% training: {len(train_sentence_10_percent)}\")\n",
    "\n",
    "label_count = pd.Series(train_label_10_percent).value_counts()\n",
    "\n",
    "print(f\"Total label '0': {label_count[0]}\")\n",
    "print(f\"Total label '1': {label_count[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "40a78236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sixth_model_use\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256830721 (979.73 MB)\n",
      "Trainable params: 32897 (128.50 KB)\n",
      "Non-trainable params: 256797824 (979.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Simply clone the sixth model as seventh model\n",
    "\n",
    "seventh_model = tf.keras.models.clone_model(sixth_model)\n",
    "\n",
    "seventh_model.compile(loss = \"binary_crossentropy\", optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = [\"accuracy\"])\n",
    "\n",
    "seventh_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "30e821bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/10_percent_tf_hub_sentence_encoder/20230924-034041\n",
      "Epoch 1/5\n",
      "22/22 [==============================] - 3s 68ms/step - loss: 0.6640 - accuracy: 0.6735 - val_loss: 0.6448 - val_accuracy: 0.6955\n",
      "Epoch 2/5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.5921 - accuracy: 0.8003 - val_loss: 0.5883 - val_accuracy: 0.7428\n",
      "Epoch 3/5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.5157 - accuracy: 0.8222 - val_loss: 0.5356 - val_accuracy: 0.7690\n",
      "Epoch 4/5\n",
      "22/22 [==============================] - 1s 24ms/step - loss: 0.4540 - accuracy: 0.8280 - val_loss: 0.5066 - val_accuracy: 0.7743\n",
      "Epoch 5/5\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.4119 - accuracy: 0.8338 - val_loss: 0.4932 - val_accuracy: 0.7782\n"
     ]
    }
   ],
   "source": [
    "seventh_model_history = seventh_model.fit(x = train_sentence_10_percent, y = train_label_10_percent,\n",
    "    epochs = 5, validation_data = (val_sentence, val_label),\n",
    "    callbacks = [create_tensorboard_callback(saved_dir_loc, \"10_percent_tf_hub_sentence_encoder\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a92c5daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.82152230971128,\n",
       " 'precision': 0.7838273596953228,\n",
       " 'recall': 0.7782152230971129,\n",
       " 'f1': 0.7747045976528383}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seventh_model_pred_prob = seventh_model.predict(val_sentence)\n",
    "seventh_model_pred = tf.squeeze(tf.round(seventh_model_pred_prob))\n",
    "\n",
    "seventh_model_result = performance_metrics(val_label, seventh_model_pred)\n",
    "seventh_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3ba6fe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 77.82, Difference: -1.443569553805773\n",
      "Baseline precision: 0.81, New precision: 0.78, Difference: -0.027311640725994457\n",
      "Baseline recall: 0.79, New recall: 0.78, Difference: -0.01443569553805768\n",
      "Baseline f1: 0.79, New f1: 0.77, Difference: -0.011514378152116644\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_with_new_result(baseline_result, seventh_model_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
