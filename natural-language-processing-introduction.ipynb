{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8558bb5",
   "metadata": {},
   "source": [
    "## Natural Language Processing with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adfaa959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from helper_function import performance_metrics, compare_baseline_with_new_result\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b8515",
   "metadata": {},
   "source": [
    "### 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09f6b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Link folders\n",
    "\n",
    "train_df = pd.read_csv(\"data/nlp/train.csv\")\n",
    "test_df = pd.read_csv(\"data/nlp/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e00eaef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Shuffle training dataframe\n",
    "\n",
    "train_df_shuffled = train_df.sample(frac = 1, random_state = 42)\n",
    "\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fe34b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 7613\n",
      "Total test samples: 3263\n",
      "Total samples: 10876\n"
     ]
    }
   ],
   "source": [
    "### How many total sample?\n",
    "\n",
    "print(f\"Total training samples: {len(train_df)}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")\n",
    "print(f\"Total samples: {len(train_df) + len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83f7614c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Please recover from the Typhoon. ????\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "The real question is why is the tornado siren going off in Dyersburg?\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "#Colorado #News Motorcyclist bicyclist injured in Denver collision on Broadway: At least two people were tak... http://t.co/2iAFPmqJeP\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Wreckage 'Conclusively Confirmed' as From MH370: Malaysia PM: Investigators and the families of those who were... http://t.co/cs8mYAunA4\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "USFS an acronym for United States Fire Service. http://t.co/8NAdrGr4xC\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Visualize random training samples\n",
    "\n",
    "random_index = random.randint(0, len(train_df) - 5)\n",
    "\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index + 5].itertuples():\n",
    "    _, text, target = row\n",
    "    \n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fef15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split training data into training and validation sets\n",
    "\n",
    "train_sentence, val_sentence, train_label, val_label = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                            test_size = 0.1,\n",
    "                                                                            random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "488c1e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train sentence: 6851, length of train label: 6851\n",
      "length of val sentence: 762, length of val label: 762\n"
     ]
    }
   ],
   "source": [
    "### Check the length of validation & training\n",
    "\n",
    "print(f\"Length of train sentence: {len(train_sentence)}, length of train label: {len(train_label)}\")\n",
    "print(f\"Length of val sentence: {len(val_sentence)}, length of val label: {len(val_label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d0c0280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first 10 training sentences and its label\n",
    "\n",
    "train_sentence[:10], train_label[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a153924",
   "metadata": {},
   "source": [
    "### 2. Converting text into numbers\n",
    "Tokenization - word-level tokenization, character-level tokenization, sub-word tokenization <br>\n",
    "Embeddings - own/ custom embedding, pre-learned embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7840d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example of text vectorization\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens = None,\n",
    "                                    standardize = \"lower_and_strip_punctuation\",\n",
    "                                    split = \"whitespace\",\n",
    "                                    ngrams = None,\n",
    "                                    output_mode = \"int\",\n",
    "                                    output_sequence_length = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "316347b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of tokens is 15\n"
     ]
    }
   ],
   "source": [
    "### What is average number of tokens (words)?\n",
    "\n",
    "print(f\"The average of tokens is {round(sum(len(i.split()) for i in train_sentences) / len(train_sentences))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b9aa4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set text vectorization with custom variables\n",
    "### Set max number of words to have in our vocabulary\n",
    "### Max length for the sequences\n",
    "\n",
    "max_vocab_length = 10000\n",
    "max_length = 15\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens = max_vocab_length,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2435e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit the text vectorizer to the training text\n",
    "\n",
    "text_vectorizer.adapt(train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93a0d995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create sample sentence and tokenize it\n",
    "### Check the output\n",
    "\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73fda714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "#USGS M 1.4 - 4km E of Interlaken California: Time2015-08-06 00:52:25 UTC2015-08-05 17:52:25 -07:00 at ep... http://t.co/zqrcptLrUM #SM      \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[1452,  772, 1444, 6321, 1300,    6, 5352,   90, 1580, 6382, 1242,\n",
       "        6360, 6377,   17, 1865]], dtype=int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Choose random sentence from the training dataset then tokenize it\n",
    "\n",
    "random_sentence = random.choice(train_sentence)\n",
    "\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nVectorized version:\")\n",
    "\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8687918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "### Get the unique words in the vocabulary\n",
    "\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5]\n",
    "bottom_5_words = words_in_vocab[-5:]\n",
    "\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\") \n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06403205",
   "metadata": {},
   "source": [
    "### 3. Creating embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe200d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating embedding layer\n",
    "embedding = layers.Embedding(input_dim = max_vocab_length,\n",
    "                             output_dim = 128,\n",
    "                             embeddings_initializer = \"uniform\",\n",
    "                             input_length = max_length,\n",
    "                             name = \"embedding_layer\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad98e3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "http://t.co/9k1tqsAarM Suicide bomber kills 15 in Saudi security site mosque - Reuters http://t.co/Ev3nX9scx3      \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.00185242,  0.00072893, -0.0376213 , ...,  0.01914451,\n",
       "          0.03461095, -0.02687767],\n",
       "        [-0.0210801 , -0.0275659 , -0.0312755 , ...,  0.02100715,\n",
       "         -0.01798378,  0.02837929],\n",
       "        [ 0.02314771,  0.03732182,  0.00252379, ..., -0.02452084,\n",
       "          0.02583529,  0.02132055],\n",
       "        ...,\n",
       "        [-0.01057015,  0.00640609, -0.02170891, ...,  0.01703012,\n",
       "         -0.01875156, -0.02419301],\n",
       "        [-0.01057015,  0.00640609, -0.02170891, ...,  0.01703012,\n",
       "         -0.01875156, -0.02419301],\n",
       "        [-0.01057015,  0.00640609, -0.02170891, ...,  0.01703012,\n",
       "         -0.01875156, -0.02419301]]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get a random sentence from training set\n",
    "### Embed the random sentence\n",
    "\n",
    "random_sentence = random.choice(train_sentence)\n",
    "\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nEmbedded version:\")\n",
    "\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf0cd265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([-1.85241550e-03,  7.28927553e-04, -3.76212969e-02,  7.56398588e-03,\n",
       "        2.06191428e-02, -2.68066768e-02, -8.39536265e-03,  1.41359605e-02,\n",
       "       -3.15573588e-02,  3.24479975e-02, -3.53546739e-02, -3.13735381e-02,\n",
       "        1.58467554e-02,  2.64225341e-02, -2.73786075e-02, -1.22269280e-02,\n",
       "        7.41760805e-03, -4.99799363e-02,  3.61898579e-02, -1.32665150e-02,\n",
       "       -3.72347981e-03,  6.82451576e-03, -3.90610695e-02,  4.50167097e-02,\n",
       "        1.37189887e-02, -2.20862031e-02, -4.78941575e-02, -2.75693890e-02,\n",
       "        1.08556636e-02, -2.44153496e-02,  4.02749069e-02, -4.79562990e-02,\n",
       "       -4.42538261e-02,  4.76114191e-02, -3.84174809e-02,  1.25062130e-02,\n",
       "       -9.49501991e-05, -3.03765889e-02,  1.82464235e-02,  1.42831728e-03,\n",
       "       -2.20111366e-02, -4.74976562e-02,  3.74122709e-03,  1.74755789e-02,\n",
       "        4.16668542e-02, -2.20945477e-02, -3.14154848e-02, -6.96597248e-03,\n",
       "        3.63042206e-03,  2.40446068e-02, -4.19338234e-02, -4.08679619e-02,\n",
       "        2.43301876e-02, -3.93568762e-02,  1.19273439e-02, -4.99103926e-02,\n",
       "        4.76809256e-02, -3.18840146e-02, -3.90865207e-02,  3.16790380e-02,\n",
       "        3.10197510e-02,  4.04576994e-02, -1.78075321e-02, -4.83571291e-02,\n",
       "       -8.14504549e-03, -9.62455198e-03,  8.07077810e-03, -1.45542137e-02,\n",
       "       -4.92032282e-02,  1.94366314e-02,  1.42928995e-02, -2.14625597e-02,\n",
       "        1.81808211e-02,  4.49131839e-02,  1.12676136e-02, -4.52122577e-02,\n",
       "       -3.57760191e-02,  1.15871541e-02,  1.48651116e-02, -4.03466448e-02,\n",
       "        4.81492393e-02, -4.55032252e-02, -6.52233511e-03,  4.35195602e-02,\n",
       "        2.30680816e-02, -3.18339691e-02, -7.01735169e-03, -4.39475290e-02,\n",
       "       -4.97688539e-02,  3.94247286e-02,  1.93329342e-02,  3.43799256e-02,\n",
       "       -4.24616337e-02, -5.46157360e-04,  4.97990139e-02, -1.39895082e-02,\n",
       "       -5.77969477e-03,  4.04491536e-02, -1.28935464e-02,  5.83878905e-03,\n",
       "        4.87206616e-02, -4.57557701e-02, -2.02635881e-02,  3.01577710e-02,\n",
       "       -4.44695242e-02, -2.37722993e-02, -3.38685066e-02, -2.77014263e-02,\n",
       "       -1.66440830e-02,  1.68486573e-02, -6.59822300e-03,  4.74586226e-02,\n",
       "        3.10340039e-02, -2.49094963e-02, -2.90145762e-02,  4.24966104e-02,\n",
       "        3.30059417e-02, -3.89407277e-02,  1.15057454e-02, -3.35270166e-03,\n",
       "        3.44927572e-02,  2.97602154e-02, -2.54077800e-02, -4.40946817e-02,\n",
       "       -4.63104136e-02,  1.91445090e-02,  3.46109532e-02, -2.68776659e-02],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Single token's embedding\n",
    "\n",
    "sample_embed[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8182de5b",
   "metadata": {},
   "source": [
    "### 4. Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0954f4",
   "metadata": {},
   "source": [
    "Model 1 - Naive Bayes (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e060ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize constant variables\n",
    "saved_dir_loc = \"model_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3cd6afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Convert words to numbers using tfidf then model the text\n",
    "\n",
    "first_model = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "first_model.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3f4b881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check performance metrics\n",
    "baseline_pred = first_model.predict(val_sentences)\n",
    "\n",
    "baseline_result = performance_metrics(y_true = val_labels, y_pred = baseline_pred)\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522fffb0",
   "metadata": {},
   "source": [
    "Model 2 - Simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce5baa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_layer (Embedding  (None, 15, 128)           1280000   \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1280129 (4.88 MB)\n",
      "Trainable params: 1280129 (4.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Create one dimensional strings inputs \n",
    "input_layer = layers.Input(shape = (1,), dtype = \"string\")\n",
    "\n",
    "### Turn the input text into numbers\n",
    "x = text_vectorizer(input_layer)\n",
    "\n",
    "### Embedding the numerized numbers\n",
    "x = embedding(x)\n",
    "\n",
    "### Lower the dimensionality of the embedding\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "### Create the output layer for binary outputs \n",
    "output_layer = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "### Construct the model\n",
    "second_model = tf.keras.Model(input_layer, output_layer)\n",
    "\n",
    "second_model.compile(loss = \"binary_crossentropy\", optimizer = tf.keras.optimizers.Adam(),\n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "second_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53d56536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/simple_dense_model/20230923-231642\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 23ms/step - loss: 0.6103 - accuracy: 0.6913 - val_loss: 0.5368 - val_accuracy: 0.7533\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.4416 - accuracy: 0.8184 - val_loss: 0.4706 - val_accuracy: 0.7874\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.3468 - accuracy: 0.8605 - val_loss: 0.4573 - val_accuracy: 0.7940\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.2840 - accuracy: 0.8918 - val_loss: 0.4689 - val_accuracy: 0.7927\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.2369 - accuracy: 0.9134 - val_loss: 0.4902 - val_accuracy: 0.7913\n"
     ]
    }
   ],
   "source": [
    "second_model_history = second_model.fit(train_sentences, train_labels, epochs = 5,\n",
    "    validation_data = (val_sentences, val_labels), \n",
    "    callbacks = [create_tensorboard_callback(dir_name = SAVE_DIR, experiment_name = \"simple_dense_model\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0143bb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4902 - accuracy: 0.7913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4901529550552368, 0.7913385629653931]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check validation results\n",
    "second_model.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0f39266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_layer/embeddings:0' shape=(10000, 128) dtype=float32, numpy=\n",
       " array([[ 0.0060296 ,  0.02295903, -0.04056848, ...,  0.03535229,\n",
       "         -0.00143927, -0.00938205],\n",
       "        [ 0.00577053,  0.0071948 , -0.04605361, ...,  0.02683257,\n",
       "          0.04248062, -0.0252433 ],\n",
       "        [ 0.00740892,  0.01506983, -0.02879825, ..., -0.00576946,\n",
       "         -0.01646395,  0.0675428 ],\n",
       "        ...,\n",
       "        [-0.02877762, -0.03890579, -0.024528  , ..., -0.00092606,\n",
       "         -0.03448058,  0.01577456],\n",
       "        [-0.00516727,  0.06183866, -0.02421062, ...,  0.07621562,\n",
       "          0.01492765,  0.00807954],\n",
       "        [ 0.10762745,  0.0228096 , -0.10485741, ...,  0.07033586,\n",
       "          0.08096483,  0.03530077]], dtype=float32)>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check embedding weights\n",
    "embedding.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e70b260d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 128)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Other way to check embedding weights\n",
    "embed_weights = second_model.get_layer(\"embedding_layer\").get_weights()[0]\n",
    "\n",
    "embed_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61b91198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard dev upload --logdir ./model_logs \\\n",
    "#   --name \"First deep model on text data\" \\\n",
    "#   --description \"Trying a dense model with an embedding layer\" \\\n",
    "#   --one_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a25a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard dev delete --experiment_id EXPERIMENT_ID_TO_DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "669e98a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.13385826771653,\n",
       " 'precision': 0.7997458316766562,\n",
       " 'recall': 0.7913385826771654,\n",
       " 'f1': 0.7874035967950923}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_model_pred_prob = second_model.predict(val_sentences)\n",
    "\n",
    "### Turn into single-dimension tensor of float\n",
    "second_model_pred = tf.squeeze(tf.round(second_model_pred_prob))\n",
    "\n",
    "second_model_result = performance_metrics(y_true = val_labels, y_pred = second_model_pred)\n",
    "\n",
    "second_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68ef5615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 79.13, Difference: -0.13123359580052352\n",
      "Baseline precision: 0.81, New precision: 0.80, Difference: -0.011393168744661009\n",
      "Baseline recall: 0.79, New recall: 0.79, Difference: -0.001312335958005173\n",
      "Baseline f1: 0.79, New f1: 0.79, Difference: 0.001184620990137386\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_with_new_result(baseline_result = baseline_result, new_result = second_model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a64bcc",
   "metadata": {},
   "source": [
    "Model 3 - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cabe5aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"third_model_lstm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " third_embedding_layer (Emb  (None, 15, 128)           1280000   \n",
      " edding)                                                         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1329473 (5.07 MB)\n",
      "Trainable params: 1329473 (5.07 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "third_model_embedding = layers.Embedding(input_dim = max_vocab_length, output_dim = 128,\n",
    "    embeddings_initializer = \"uniform\", input_length = max_length, name = \"third_embedding_layer\")\n",
    "\n",
    "input_layer = layers.Input(shape = (1,), dtype = \"string\")\n",
    "\n",
    "x = text_vectorizer(input_layer)\n",
    "x = third_model_embedding(x)\n",
    "# print(x.shape)\n",
    "x = layers.LSTM(64)(x)\n",
    "# print(x.shape)\n",
    "\n",
    "output_layer = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "third_model = tf.keras.Model(input_layer, output_layer, name = \"third_model_lstm\")\n",
    "\n",
    "third_model.compile(loss = \"binary_crossentropy\", optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = [\"accuracy\"])\n",
    "\n",
    "third_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a81fdc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/lstm/20230923-231707\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 9s 28ms/step - loss: 0.5098 - accuracy: 0.7505 - val_loss: 0.4613 - val_accuracy: 0.7835\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 6s 28ms/step - loss: 0.3171 - accuracy: 0.8716 - val_loss: 0.4738 - val_accuracy: 0.7782\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 6s 27ms/step - loss: 0.2189 - accuracy: 0.9199 - val_loss: 0.5906 - val_accuracy: 0.7743\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 6s 26ms/step - loss: 0.1534 - accuracy: 0.9442 - val_loss: 0.6188 - val_accuracy: 0.7454\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 6s 27ms/step - loss: 0.1125 - accuracy: 0.9591 - val_loss: 0.9497 - val_accuracy: 0.7598\n"
     ]
    }
   ],
   "source": [
    "third_model_history = third_model.fit(train_sentences, train_labels, epochs = 5, \n",
    "    validation_data = (val_sentences, val_labels), callbacks = [create_tensorboard_callback(SAVE_DIR, \"lstm\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5beb3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard dev upload --logdir ./model_logs \\\n",
    "#   --name \"First deep model on text data\" \\\n",
    "#   --description \"Trying a dense model with an embedding layer\" \\\n",
    "#   --one_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fce5ac14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 75.98425196850394,\n",
       " 'precision': 0.7637560697167074,\n",
       " 'recall': 0.7598425196850394,\n",
       " 'f1': 0.7563819709955472}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_model_pred_prob = third_model.predict(val_sentences)\n",
    "third_model_pred = tf.squeeze(tf.round(third_model_pred_prob))\n",
    "\n",
    "third_model_result = performance_metrics(y_true = val_labels, y_pred = third_model_pred)\n",
    "third_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c13d57f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 75.98, Difference: -3.2808398950131163\n",
      "Baseline precision: 0.81, New precision: 0.76, Difference: -0.04738293070460986\n",
      "Baseline recall: 0.79, New recall: 0.76, Difference: -0.03280839895013121\n",
      "Baseline f1: 0.79, New f1: 0.76, Difference: -0.029837004809407763\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_with_new_result(baseline_result, third_model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a0d4a2",
   "metadata": {},
   "source": [
    "Model 4 - Bidirectonal RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1eeb2b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"forth_model_bidirectional_rnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " forth_embedding (Embedding  (None, 15, 128)           1280000   \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 128)               98816     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1378945 (5.26 MB)\n",
      "Trainable params: 1378945 (5.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "forth_model_embedding = layers.Embedding(input_dim = max_vocab_length, output_dim = 128,\n",
    "    embeddings_initializer = \"uniform\", input_length = max_length, name = \"forth_embedding\")\n",
    "\n",
    "input_layer = layers.Input(shape = (1,), dtype = \"string\")\n",
    "\n",
    "x = text_vectorizer(input_layer)\n",
    "x = forth_model_embedding(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "\n",
    "output_layer = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "forth_model = tf.keras.Model(input_layer, output_layer, name = \"forth_model_bidirectional_rnn\")\n",
    "\n",
    "forth_model.compile(loss = \"binary_crossentropy\", optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = [\"accuracy\"])\n",
    "\n",
    "forth_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "374f8f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/bidirectional_rnn/20230923-231740\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 10s 30ms/step - loss: 0.5090 - accuracy: 0.7463 - val_loss: 0.4560 - val_accuracy: 0.7874\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 6s 27ms/step - loss: 0.3101 - accuracy: 0.8740 - val_loss: 0.4825 - val_accuracy: 0.7782\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 6s 26ms/step - loss: 0.2052 - accuracy: 0.9225 - val_loss: 0.6169 - val_accuracy: 0.7703\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 5s 25ms/step - loss: 0.1349 - accuracy: 0.9527 - val_loss: 0.6960 - val_accuracy: 0.7651\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 5s 25ms/step - loss: 0.0925 - accuracy: 0.9657 - val_loss: 0.9134 - val_accuracy: 0.7638\n"
     ]
    }
   ],
   "source": [
    "forth_model_history = forth_model.fit(train_sentences, train_labels,\n",
    "    epochs = 5, validation_data = (val_sentences, val_labels), \n",
    "    callbacks = [create_tensorboard_callback(SAVE_DIR, \"bidirectional_rnn\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91198b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.37795275590551,\n",
       " 'precision': 0.7702681471235864,\n",
       " 'recall': 0.7637795275590551,\n",
       " 'f1': 0.7594175807340501}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forth_model_pred_prob = forth_model.predict(val_sentences)\n",
    "forth_model_pred = tf.squeeze(tf.round(forth_model_pred_prob))\n",
    "\n",
    "forth_model_result = performance_metrics(val_labels, forth_model_pred)\n",
    "forth_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ee0fd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 76.38, Difference: -2.887139107611546\n",
      "Baseline precision: 0.81, New precision: 0.77, Difference: -0.04087085329773088\n",
      "Baseline recall: 0.79, New recall: 0.76, Difference: -0.02887139107611547\n",
      "Baseline f1: 0.79, New f1: 0.76, Difference: -0.026801395070904843\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_with_new_result(baseline_result, forth_model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef67620",
   "metadata": {},
   "source": [
    "Model 5 - One dimensional cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2d69a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fifth_model_cnn_1d\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " fifth_embedding (Embedding  (None, 15, 128)           1280000   \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 11, 32)            20512     \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 32)                0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1300545 (4.96 MB)\n",
      "Trainable params: 1300545 (4.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fifth_model_embedding = layers.Embedding(input_dim = max_vocab_length, output_dim = 128,\n",
    "    embeddings_initializer = \"uniform\", input_length = max_length, name = \"fifth_embedding\")\n",
    "\n",
    "input_layer = layers.Input(shape = (1,), dtype = \"string\")\n",
    "\n",
    "x = text_vectorizer(input_layer)\n",
    "x = fifth_model_embedding(x)\n",
    "x = layers.Conv1D(filters = 32, kernel_size = 5, activation = \"relu\")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "\n",
    "output_layer = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "fifth_model = tf.keras.Model(input_layer, output_layer, name = \"fifth_model_cnn_1d\")\n",
    "\n",
    "fifth_model.compile(loss = \"binary_crossentropy\", \n",
    "    optimizer = tf.keras.optimizers.Adam(), metrics = [\"accuracy\"])\n",
    "\n",
    "fifth_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c23ee67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/cnn_1d/20230923-231813\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.5652 - accuracy: 0.7199 - val_loss: 0.4748 - val_accuracy: 0.7743\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 20ms/step - loss: 0.3480 - accuracy: 0.8571 - val_loss: 0.4840 - val_accuracy: 0.7848\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.2198 - accuracy: 0.9204 - val_loss: 0.5512 - val_accuracy: 0.7769\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.1388 - accuracy: 0.9539 - val_loss: 0.6160 - val_accuracy: 0.7703\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 4s 20ms/step - loss: 0.0990 - accuracy: 0.9688 - val_loss: 0.6664 - val_accuracy: 0.7651\n"
     ]
    }
   ],
   "source": [
    "fifth_model_history = fifth_model.fit(train_sentences, train_labels,\n",
    "    epochs = 5, validation_data = (val_sentences, val_labels),\n",
    "    callbacks = [create_tensorboard_callback(SAVE_DIR, \"cnn_1d\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13be68ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.50918635170603,\n",
       " 'precision': 0.765325307490926,\n",
       " 'recall': 0.7650918635170604,\n",
       " 'f1': 0.7637474572934756}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fifth_model_pred_prob = fifth_model.predict(val_sentences)\n",
    "fifth_model_pred = tf.squeeze(tf.round(fifth_model_pred_prob))\n",
    "\n",
    "fifth_model_result = performance_metrics(val_labels, fifth_model_pred)\n",
    "fifth_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22dfc792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 76.51, Difference: -2.7559055118110223\n",
      "Baseline precision: 0.81, New precision: 0.77, Difference: -0.04581369293039128\n",
      "Baseline recall: 0.79, New recall: 0.77, Difference: -0.027559055118110187\n",
      "Baseline f1: 0.79, New f1: 0.76, Difference: -0.022471518511479327\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_with_new_result(baseline_result, fifth_model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9687a4e",
   "metadata": {},
   "source": [
    "Model 6 - Pretrained sentence encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8c56eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sixth_model_use\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256830721 (979.73 MB)\n",
      "Trainable params: 32897 (128.50 KB)\n",
      "Non-trainable params: 256797824 (979.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "    input_shape = [], dtype = tf.string, trainable = False, name = \"USE\")\n",
    "\n",
    "sixth_model = tf.keras.Sequential([\n",
    "  sentence_encoder_layer,\n",
    "  layers.Dense(64, activation = \"relu\"),\n",
    "  layers.Dense(1, activation = \"sigmoid\")\n",
    "], name = \"sixth_model_use\")\n",
    "\n",
    "sixth_model.compile(loss = \"binary_crossentropy\",\n",
    "                optimizer = tf.keras.optimizers.Adam(),\n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "sixth_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7d12cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/tf_hub_sentence_encoder/20230923-232702\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 7s 20ms/step - loss: 0.5003 - accuracy: 0.7885 - val_loss: 0.4491 - val_accuracy: 0.7940\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.4146 - accuracy: 0.8133 - val_loss: 0.4411 - val_accuracy: 0.8097\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3999 - accuracy: 0.8228 - val_loss: 0.4313 - val_accuracy: 0.8150\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3911 - accuracy: 0.8288 - val_loss: 0.4332 - val_accuracy: 0.8150\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3851 - accuracy: 0.8305 - val_loss: 0.4248 - val_accuracy: 0.8150\n"
     ]
    }
   ],
   "source": [
    "sixth_model_history = sixth_model.fit(train_sentences, train_labels,\n",
    "    epochs = 5, validation_data = (val_sentences, val_labels), \n",
    "    callbacks=[create_tensorboard_callback(SAVE_DIR, \"tf_hub_sentence_encoder\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9fb07507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 81.49606299212599,\n",
       " 'precision': 0.8169586293569981,\n",
       " 'recall': 0.8149606299212598,\n",
       " 'f1': 0.8135344618830033}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sixth_model_pred_prob = sixth_model.predict(val_sentences)\n",
    "sixth_model_pred = tf.squeeze(tf.round(sixth_model_pred_prob))\n",
    "\n",
    "sixth_model_result = performance_metrics(val_labels, sixth_model_pred)\n",
    "sixth_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc00c5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 81.50, Difference: 2.230971128608928\n",
      "Baseline precision: 0.81, New precision: 0.82, Difference: 0.005819628935680887\n",
      "Baseline recall: 0.79, New recall: 0.81, Difference: 0.022309711286089273\n",
      "Baseline f1: 0.79, New f1: 0.81, Difference: 0.027315486078048345\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_with_new_result(baseline_result, sixth_model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c1f29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
