{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adfaa959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b8515",
   "metadata": {},
   "source": [
    "### 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f6b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Link folders\n",
    "\n",
    "train_df = pd.read_csv(\"data/nlp/train.csv\")\n",
    "test_df = pd.read_csv(\"data/nlp/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e00eaef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Shuffle training dataframe\n",
    "\n",
    "train_df_shuffled = train_df.sample(frac = 1, random_state = 42)\n",
    "\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe34b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 7613\n",
      "Total test samples: 3263\n",
      "Total samples: 10876\n"
     ]
    }
   ],
   "source": [
    "### How many total sample?\n",
    "\n",
    "print(f\"Total training samples: {len(train_df)}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")\n",
    "print(f\"Total samples: {len(train_df) + len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83f7614c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Still can't get over the thunderstorm/tornado we were woken up to yesterday. Half the street is still in the dark! http://t.co/Y8h5v1j2y7\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Wreck with road blockage Woodward Avenue Northbound at Davison in M.S. #shoalstraffic\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Like it affects every level of life you're expecting me to buy everything and still survive with my limited pocket money\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "#WorldNews Fallen powerlines on G:link tram: UPDATE: FIRE crews have evacuated up to 30 passengers who were tr... http://t.co/EYSVvzA7Qm\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Has gun law ever dissuaded a potential mass murderer?\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Visualize random training samples\n",
    "\n",
    "random_index = random.randint(0, len(train_df) - 5)\n",
    "\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index + 5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fef15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split training data into training and validation sets\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                            test_size = 0.1,\n",
    "                                                                            random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "488c1e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check the length of validation & training\n",
    "\n",
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d0c0280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first 10 training sentences and its label\n",
    "\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a153924",
   "metadata": {},
   "source": [
    "### 2. Converting text into numbers\n",
    "Tokenization - word-level tokenization, character-level tokenization, sub-word tokenization <br>\n",
    "Embeddings - own/ custom embedding, pre-learned embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7840d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example of text vectorization\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens = None,\n",
    "                                    standardize = \"lower_and_strip_punctuation\",\n",
    "                                    split = \"whitespace\",\n",
    "                                    ngrams = None,\n",
    "                                    output_mode = \"int\",\n",
    "                                    output_sequence_length = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "316347b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### What is average number of tokens (words)?\n",
    "\n",
    "round(sum(len(i.split()) for i in train_sentences) / len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b9aa4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set text vectorization with custom variables\n",
    "### Set max number of words to have in our vocabulary\n",
    "### Max length for the sequences\n",
    "\n",
    "max_vocab_length = 10000\n",
    "max_length = 15\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens = max_vocab_length,\n",
    "                                    output_mode = \"int\",\n",
    "                                    output_sequence_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2435e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit the text vectorizer to the training text\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93a0d995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create sample sentence and tokenize it\n",
    "### Check the output\n",
    "\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73fda714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Love waking up to my dad screaming at me ??????      \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 110, 4291,   27,    5,   13, 1419,  311,   17,   31,    0,    0,\n",
       "           0,    0,    0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Choose random sentence from the training dataset then tokenize it\n",
    "\n",
    "random_sentence = random.choice(train_sentences)\n",
    "\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nVectorized version:\")\n",
    "\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8687918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "### Get the unique words in the vocabulary\n",
    "\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5]\n",
    "bottom_5_words = words_in_vocab[-5:]\n",
    "\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\") \n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06403205",
   "metadata": {},
   "source": [
    "### 3. Creating embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe200d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating embedding layer\n",
    "\n",
    "embedding = layers.Embedding(input_dim = max_vocab_length,\n",
    "                             output_dim = 128,\n",
    "                             embeddings_initializer = \"uniform\",\n",
    "                             input_length = max_length,\n",
    "                             name = \"embedding_layer\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad98e3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "I just wanted to watch Paper Towns but the buildings on fire ?????      \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[ 0.04301525, -0.04443644, -0.01748439, ...,  0.04449774,\n",
       "         -0.00153724, -0.04439235],\n",
       "        [ 0.04555226,  0.02510707, -0.0017382 , ...,  0.04944488,\n",
       "          0.02425803,  0.00947589],\n",
       "        [-0.03437819,  0.04130817, -0.02357488, ...,  0.03838101,\n",
       "          0.04014463,  0.00493345],\n",
       "        ...,\n",
       "        [ 0.03371259, -0.00171655,  0.02867376, ..., -0.04695725,\n",
       "         -0.03288045, -0.03945033],\n",
       "        [ 0.03371259, -0.00171655,  0.02867376, ..., -0.04695725,\n",
       "         -0.03288045, -0.03945033],\n",
       "        [ 0.03371259, -0.00171655,  0.02867376, ..., -0.04695725,\n",
       "         -0.03288045, -0.03945033]]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get a random sentence from training set\n",
    "### Embed the random sentence\n",
    "\n",
    "random_sentence = random.choice(train_sentences)\n",
    "\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nEmbedded version:\")\n",
    "\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf0cd265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([ 0.04301525, -0.04443644, -0.01748439, -0.03508688,  0.00029314,\n",
       "        0.03428726, -0.04057056, -0.04228323,  0.0082969 , -0.01319723,\n",
       "       -0.04065651,  0.01285983,  0.01793614,  0.01988634, -0.00381547,\n",
       "       -0.00659677, -0.04319429,  0.0002957 ,  0.01494164,  0.01893128,\n",
       "       -0.02990079, -0.02787136,  0.02569074,  0.04917817,  0.00226707,\n",
       "        0.00238109, -0.00410699, -0.02816108, -0.04933527,  0.01428999,\n",
       "       -0.00102276, -0.04656769,  0.01690919,  0.0202193 , -0.00054177,\n",
       "        0.00970261, -0.0277022 , -0.04200481,  0.03645274,  0.0424467 ,\n",
       "        0.00378089, -0.02008352, -0.03885863,  0.03527704,  0.02983529,\n",
       "        0.04967414,  0.01167498,  0.01129816, -0.03097321,  0.02864044,\n",
       "        0.03663311, -0.00881679, -0.02291065,  0.01550363, -0.03079291,\n",
       "       -0.04998752,  0.00915421,  0.03727027,  0.04121624, -0.03007622,\n",
       "       -0.03852018, -0.0250762 ,  0.02410323, -0.00825997,  0.04990783,\n",
       "       -0.01498093,  0.0026735 ,  0.00191538, -0.02210919,  0.04065441,\n",
       "        0.01742693, -0.04844358, -0.03359189, -0.00724395, -0.0397976 ,\n",
       "       -0.00108058,  0.00591696, -0.00297945,  0.03422091,  0.03332971,\n",
       "        0.0313496 ,  0.01069589, -0.01395184,  0.01239615, -0.04302416,\n",
       "        0.04270906,  0.02822867, -0.01986233, -0.03506731, -0.04094766,\n",
       "       -0.03055493,  0.02753601,  0.04806611,  0.02766799,  0.03658152,\n",
       "        0.00748239, -0.01442659, -0.03347816,  0.02433068, -0.04767299,\n",
       "       -0.04133359, -0.01852852,  0.04406506, -0.03924759, -0.00671619,\n",
       "        0.01924824, -0.03221785,  0.04701703, -0.02152959, -0.0385472 ,\n",
       "       -0.0305048 , -0.03246607,  0.04868272, -0.01662026,  0.04486587,\n",
       "       -0.04224167, -0.03481457, -0.01281351,  0.04344065,  0.01883204,\n",
       "       -0.03142636,  0.00808191, -0.03594854, -0.01967448,  0.04415423,\n",
       "        0.04449774, -0.00153724, -0.04439235], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Single token's embedding\n",
    "\n",
    "sample_embed[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8182de5b",
   "metadata": {},
   "source": [
    "### 4. Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0954f4",
   "metadata": {},
   "source": [
    "Model 1 - Naive Bayes (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e060ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"model_logs\"\n",
    "\n",
    "### Function for performance metrics\n",
    "\n",
    "def performance_metrics(y_true, y_pred):\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average = \"weighted\")\n",
    "    \n",
    "    model_results = {\"accuracy\": model_accuracy,\n",
    "                  \"precision\": model_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "    \n",
    "    return model_results\n",
    "\n",
    "### Function for comparing new and old result\n",
    "\n",
    "def compare_baseline_with_new_result(baseline_result, new_result):\n",
    "    for key, value in baseline_result.items():\n",
    "        print(f\"Baseline {key}: {value:.2f}, New {key}: {new_result[key]:.2f}, Difference: {new_result[key] - value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3cd6afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Convert words to numbers using tfidf then model the text\n",
    "\n",
    "first_model = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "first_model.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3f4b881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check performance metrics\n",
    "\n",
    "baseline_pred = first_model.predict(val_sentences)\n",
    "\n",
    "baseline_result = performance_metrics(y_true = val_labels, y_pred = baseline_pred)\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522fffb0",
   "metadata": {},
   "source": [
    "Model 2 - Simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce5baa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_layer (Embedding  (None, 15, 128)           1280000   \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1280129 (4.88 MB)\n",
      "Trainable params: 1280129 (4.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Create one dimensional strings inputs \n",
    "input_layer = layers.Input(shape = (1,), dtype = \"string\")\n",
    "\n",
    "### Turn the input text into numbers\n",
    "x = text_vectorizer(input_layer)\n",
    "\n",
    "### Embedding the numerized numbers\n",
    "x = embedding(x)\n",
    "\n",
    "### Lower the dimensionality of the embedding\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "### Create the output layer for binary outputs \n",
    "output_layer = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "### Construct the model\n",
    "second_model = tf.keras.Model(input_layer, output_layer)\n",
    "\n",
    "second_model.compile(loss = \"binary_crossentropy\", optimizer = tf.keras.optimizers.Adam(),\n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "second_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53d56536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/simple_dense_model/20230922-225243\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 26ms/step - loss: 0.6132 - accuracy: 0.6910 - val_loss: 0.5380 - val_accuracy: 0.7480\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 5s 25ms/step - loss: 0.4423 - accuracy: 0.8189 - val_loss: 0.4691 - val_accuracy: 0.7848\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.3472 - accuracy: 0.8608 - val_loss: 0.4563 - val_accuracy: 0.7848\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 4s 20ms/step - loss: 0.2843 - accuracy: 0.8917 - val_loss: 0.4664 - val_accuracy: 0.7927\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.2372 - accuracy: 0.9108 - val_loss: 0.4826 - val_accuracy: 0.7874\n"
     ]
    }
   ],
   "source": [
    "second_model_history = second_model.fit(train_sentences, train_labels, epochs = 5,\n",
    "    validation_data = (val_sentences, val_labels), \n",
    "    callbacks = [create_tensorboard_callback(dir_name = SAVE_DIR, experiment_name = \"simple_dense_model\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0143bb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step - loss: 0.4826 - accuracy: 0.7874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4826074540615082, 0.787401556968689]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check validation results\n",
    "\n",
    "second_model.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0f39266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_layer/embeddings:0' shape=(10000, 128) dtype=float32, numpy=\n",
       " array([[ 0.01463184, -0.01991755,  0.04797755, ..., -0.06716058,\n",
       "         -0.05348415, -0.01951163],\n",
       "        [ 0.03035338, -0.01621791, -0.03138665, ..., -0.03135433,\n",
       "          0.03413351,  0.04508649],\n",
       "        [-0.0019709 ,  0.02034102,  0.007183  , ...,  0.00484237,\n",
       "         -0.01946999, -0.01748167],\n",
       "        ...,\n",
       "        [ 0.04960455, -0.0209007 , -0.02314503, ..., -0.02859908,\n",
       "          0.04579038,  0.03502407],\n",
       "        [ 0.00269462, -0.06377188, -0.01078972, ..., -0.05997188,\n",
       "         -0.05074562,  0.05960172],\n",
       "        [-0.04528678, -0.0967545 ,  0.01804337, ..., -0.03746699,\n",
       "         -0.06067383,  0.03093776]], dtype=float32)>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check embedding weights\n",
    "\n",
    "embedding.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e70b260d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 128)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Other way to check embedding weights\n",
    "\n",
    "embed_weights = second_model.get_layer(\"embedding_layer\").get_weights()[0]\n",
    "\n",
    "embed_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61b91198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard dev upload --logdir ./model_logs \\\n",
    "#   --name \"First deep model on text data\" \\\n",
    "#   --description \"Trying a dense model with an embedding layer\" \\\n",
    "#   --one_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a25a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard dev delete --experiment_id EXPERIMENT_ID_TO_DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "669e98a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.74015748031496,\n",
       " 'precision': 0.7937136229340627,\n",
       " 'recall': 0.7874015748031497,\n",
       " 'f1': 0.7839588199365206}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_model_pred_prob = second_model.predict(val_sentences)\n",
    "\n",
    "### Turn into single-dimension tensor of float\n",
    "second_model_pred = tf.squeeze(tf.round(second_model_pred_prob))\n",
    "\n",
    "second_model_result = performance_metrics(y_true = val_labels, y_pred = second_model_pred)\n",
    "\n",
    "second_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73cd1800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 78.74, Difference: -0.5249343832020941\n",
      "Baseline precision: 0.81, New precision: 0.79, Difference: -0.01742537748725459\n",
      "Baseline recall: 0.79, New recall: 0.79, Difference: -0.005249343832020914\n",
      "Baseline f1: 0.79, New f1: 0.78, Difference: -0.0022601558684343104\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_with_new_result(baseline_result = baseline_result, new_result = second_model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a64bcc",
   "metadata": {},
   "source": [
    "Model 3 - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cabe5aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"third_model_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " third_embedding_layer (Emb  (None, 15, 128)           1280000   \n",
      " edding)                                                         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1329473 (5.07 MB)\n",
      "Trainable params: 1329473 (5.07 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "third_model_embedding = layers.Embedding(input_dim = max_vocab_length, output_dim = 128,\n",
    "    embeddings_initializer = \"uniform\", input_length = max_length, name = \"third_embedding_layer\")\n",
    "\n",
    "input_layer = layers.Input(shape = (1,), dtype = \"string\")\n",
    "\n",
    "x = text_vectorizer(input_layer)\n",
    "x = third_model_embedding(x)\n",
    "# print(x.shape)\n",
    "x = layers.LSTM(64)(x)\n",
    "# print(x.shape)\n",
    "\n",
    "output_layer = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "third_model = tf.keras.Model(input_layer, output_layer, name = \"third_model_lstm\")\n",
    "\n",
    "third_model.compile(loss = \"binary_crossentropy\", optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = [\"accuracy\"])\n",
    "\n",
    "third_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a81fdc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/LSTM/20230922-225309\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 7s 22ms/step - loss: 0.5109 - accuracy: 0.7495 - val_loss: 0.4768 - val_accuracy: 0.7756\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.3154 - accuracy: 0.8732 - val_loss: 0.5073 - val_accuracy: 0.7743\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.2205 - accuracy: 0.9149 - val_loss: 0.6025 - val_accuracy: 0.7703\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 6s 29ms/step - loss: 0.1532 - accuracy: 0.9428 - val_loss: 0.5839 - val_accuracy: 0.7743\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1039 - accuracy: 0.9629 - val_loss: 0.8873 - val_accuracy: 0.7598\n"
     ]
    }
   ],
   "source": [
    "third_model_history = third_model.fit(train_sentences, train_labels, epochs = 5, \n",
    "    validation_data = (val_sentences, val_labels), callbacks = [create_tensorboard_callback(SAVE_DIR, \"lstm\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5beb3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard dev upload --logdir ./model_logs \\\n",
    "#   --name \"First deep model on text data\" \\\n",
    "#   --description \"Trying a dense model with an embedding layer\" \\\n",
    "#   --one_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "842ce6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 75.98425196850394,\n",
       " 'precision': 0.7598710707718088,\n",
       " 'recall': 0.7598425196850394,\n",
       " 'f1': 0.758578322737536}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_model_pred_prob = third_model.predict(val_sentences)\n",
    "third_model_pred = tf.squeeze(tf.round(third_model_pred_prob))\n",
    "\n",
    "third_model_result = performance_metrics(y_true = val_labels, y_pred = third_model_pred)\n",
    "third_model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19106843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 75.98, Difference: -3.2808398950131163\n",
      "Baseline precision: 0.81, New precision: 0.76, Difference: -0.05126792964950844\n",
      "Baseline recall: 0.79, New recall: 0.76, Difference: -0.03280839895013121\n",
      "Baseline f1: 0.79, New f1: 0.76, Difference: -0.02764065306741892\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_with_new_result(baseline_result, third_model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43cbfcb",
   "metadata": {},
   "source": [
    "Model 4 - Bidirectonal RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a770809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"forth_model_bidirectional_rnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " forth_embedding (Embedding  (None, 15, 128)           1280000   \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 128)               98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1378945 (5.26 MB)\n",
      "Trainable params: 1378945 (5.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "forth_model_embedding = layers.Embedding(input_dim = max_vocab_length, output_dim = 128,\n",
    "    embeddings_initializer = \"uniform\", input_length = max_length, name = \"forth_embedding\")\n",
    "\n",
    "input_layer = layers.Input(shape = (1,), dtype = \"string\")\n",
    "\n",
    "x = text_vectorizer(input_layer)\n",
    "x = forth_model_embedding(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "\n",
    "output_layer = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "forth_model = tf.keras.Model(input_layer, output_layer, name = \"forth_model_bidirectional_rnn\")\n",
    "\n",
    "forth_model.compile(loss = \"binary_crossentropy\", optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = [\"accuracy\"])\n",
    "\n",
    "forth_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5a3728f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/bidirectional_rnn/20230922-231811\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 9s 26ms/step - loss: 0.5074 - accuracy: 0.7470 - val_loss: 0.4633 - val_accuracy: 0.7795\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.3089 - accuracy: 0.8749 - val_loss: 0.4856 - val_accuracy: 0.7874\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 6s 26ms/step - loss: 0.2049 - accuracy: 0.9250 - val_loss: 0.5687 - val_accuracy: 0.7677\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 5s 25ms/step - loss: 0.1397 - accuracy: 0.9533 - val_loss: 0.6408 - val_accuracy: 0.7703\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 5s 25ms/step - loss: 0.0951 - accuracy: 0.9658 - val_loss: 0.8945 - val_accuracy: 0.7493\n"
     ]
    }
   ],
   "source": [
    "forth_model_history = forth_model.fit(train_sentences, train_labels,\n",
    "    epochs = 5, validation_data = (val_sentences, val_labels), \n",
    "    callbacks = [create_tensorboard_callback(SAVE_DIR, \"bidirectional_rnn\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a1aabb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 74.93438320209974,\n",
       " 'precision': 0.7490502271995707,\n",
       " 'recall': 0.7493438320209974,\n",
       " 'f1': 0.7482434768818874}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forth_model_pred_prob = forth_model.predict(val_sentences)\n",
    "forth_model_pred = tf.squeeze(tf.round(forth_model_pred_prob))\n",
    "\n",
    "forth_model_results = performance_metrics(val_labels, forth_model_pred)\n",
    "forth_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a9c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_baseline_to_new_results(baseline_results, model_4_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
